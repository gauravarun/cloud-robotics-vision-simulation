# ğŸ§  Cloud-Based Robotics Vision Simulation (PyBullet + ROS)

This project showcases two core robotics vision tasksâ€”**shape classification** and **vision-guided autonomous navigation**â€”implemented fully in cloud environments using **PyBullet**, **ROS**, **OpenCV**, and **TheConstruct.ai**. It demonstrates both **passive perception** (understanding images) and **active perception** (using vision to act in real-time).

---

## ğŸ“Œ Project Overview

This project consists of two major tasks:

### **ğŸ”¹ Task 1: Robot Vision Simulation in PyBullet**

* Created geometric objects (cube, sphere, cylinder, pyramid) using **URDF**.
* Built a synthetic 3D environment and captured RGB images using virtual cameras.
* Preprocessed images (resize, normalize).
* Used a pretrained **MobileNetV2** CNN for shape classification.
* Evaluated model performance with a confusion matrix and accuracy metrics.

**Tools Used:** PyBullet, Python, MobileNetV2, OpenCV, Matplotlib, Google Colab.

---

### **ğŸ”¹ Task 2: Vision-Guided Autonomous Navigation in ROS**

* Used a **TurtleBot3** robot in a cloud ROS environment (TheConstruct.ai).
* Implemented real-time **red-object detection** using OpenCV (HSV thresholding).
* Created a custom ROS package `vision_nav` with node `color_tracker.py`.
* Robot behavior:

  * Rotate left/right depending on object position
  * Move forward when aligned
  * Stop if no object detected
* Achieved full closed-loop perception-to-action navigation.

**Tools Used:** ROS Noetic, Gazebo, OpenCV, cv_bridge, geometry_msgs, sensor_msgs.

---

## ğŸ“‚ Repository Structure

```
cloud-robotics-vision-simulation/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ task1_pybullet/
â”‚   â”œâ”€â”€ urdf/
â”‚   â”œâ”€â”€ simulation_code.ipynb
â”‚
â”œâ”€â”€ task2_ros_navigation/
â”‚   â”œâ”€â”€ vision_nav/
â”‚   â”‚   â””â”€â”€ color_tracker.py
â”‚   â””â”€â”€ screenshots/
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ ARACV_Assignment.pdf
â”‚
```

---

## ğŸ“Š Task 1 Results (PyBullet)

* Multiple camera views captured (diagonal, top-down, close-up).
* MobileNetV2 classification accuracy ~18% (expected due to ImageNet mismatch).
* Confusion matrix analysis revealed shape misclassifications and feature overlap.

**Key Insight:** Pretrained models struggle on synthetic geometric shapes without fine-tuning.

---

## ğŸ¤– Task 2 Results (ROS Navigation)

* Real-time tracking of a red object using HSV segmentation.
* TurtleBot3 successfully aligned and navigated toward the target.
* Demonstrated a complete **sense â†’ process â†’ act** pipeline.

**Limitations:**

* Hardcoded HSV values
* No obstacle avoidance
* No smoothing/filtering

**Potential Improvements:**

* Dynamic thresholding or learned detectors (YOLO, SSD)
* LIDAR-based collision avoidance
* Kalman filtering for smoother tracking
* Proportional control for smooth motion

---

## ğŸ”— Important Links

* **Task 1 (PyBullet) â€“ Google Colab:**
  [https://colab.research.google.com/drive/1en0lSX-WQLqjc4s24X8FXwSsgRO5RF6_?usp=sharing](https://colab.research.google.com/drive/1en0lSX-WQLqjc4s24X8FXwSsgRO5RF6_?usp=sharing)

* **Task 2 (ROS Navigation) â€“ ROSject:**
  *Unique ID:* FVEFV0AASI

* **Backup Files:**
  [https://drive.google.com/drive/folders/11p4VZ7Dq13C2X_Ce4knErTA_Q9aBdXZt?usp=sharing](https://drive.google.com/drive/folders/11p4VZ7Dq13C2X_Ce4knErTA_Q9aBdXZt?usp=sharing)

---

## ğŸ› ï¸ Technologies Used

* **PyBullet** (Vision simulation)
* **ROS Noetic + Gazebo** (Autonomous navigation)
* **OpenCV** (Image processing)
* **MobileNetV2 + PyTorch** (Deep learning)
* **Python 3.11**
* **TheConstruct.ai** (Cloud-based simulation)
* **Google Colab**

---

## ğŸ‘¨â€ğŸ’» Author

**Gaurav Arun**
MSc Artificial Intelligence â€“ Robotics, Automation & Computer Vision
2025


